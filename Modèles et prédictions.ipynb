{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ca7d4f-a62b-4fea-9c24-515336c4c5ce",
   "metadata": {},
   "source": [
    "# Modèles et prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551978e-00b0-4933-be5d-1733188b6c32",
   "metadata": {},
   "source": [
    "#### 1. Préparation des données (X, y, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819b4f5-91e4-4de7-bb6e-8ab10879175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, ElasticNet, BayesianRidge\n",
    "import xgboost as xgb\n",
    "\n",
    "# Exemple si ton DataFrame s'appelle df et que la target est 'RET'\n",
    "# (True/False -> classification binaire)\n",
    "X = train.drop(columns=['RET'])\n",
    "y = train['RET'].astype(int)  # 0/1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6e634-b19b-47cf-98a3-b0ae6def4bd5",
   "metadata": {},
   "source": [
    "#### 2. Fonction utilitaire pour évaluer les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492defa6-7f27-4961-ae00-466be8b0ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Affiche Accuracy, F1 et ROC-AUC sur train et test.\n",
    "    \"\"\"\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # certaines méthodes comme XGBoost peuvent renvoyer proba ou logits\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_proba_test = model.decision_function(X_test)\n",
    "    else:\n",
    "        # fallback: proba approchée par la prédiction binaire\n",
    "        y_proba_test = y_pred_test\n",
    "\n",
    "    print(f\"====== {name} ======\")\n",
    "    print(\"Train accuracy :\", accuracy_score(y_train, y_pred_train))\n",
    "    print(\"Test  accuracy :\", accuracy_score(y_test, y_pred_test))\n",
    "    print(\"Train F1       :\", f1_score(y_train, y_pred_train))\n",
    "    print(\"Test  F1       :\", f1_score(y_test, y_pred_test))\n",
    "    print(\"Test  ROC-AUC  :\", roc_auc_score(y_test, y_proba_test))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6f24e-64a3-4618-932e-60c8de3b9f8a",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression (baseline + GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705632f4-bb24-4462-a016-7e1974aed727",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),  # with_mean=False si sparse\n",
    "    (\"clf\", LogisticRegression(max_iter=500, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_logit = {\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__solver\": [\"lbfgs\", \"saga\"]\n",
    "}\n",
    "\n",
    "grid_logit = GridSearchCV(\n",
    "    logit_pipe,\n",
    "    param_grid=param_logit,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_logit.fit(X_train, y_train)\n",
    "print(\"Best params Logistic:\", grid_logit.best_params_)\n",
    "print(\"Best CV score (F1)  :\", grid_logit.best_score_)\n",
    "\n",
    "best_logit = grid_logit.best_estimator_\n",
    "eval_model(\"Logistic Regression (GridSearch)\", best_logit, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b79a01-620d-4f4d-8044-4a89b6915091",
   "metadata": {},
   "source": [
    "#### 4. Lasso (classification via régression logistique L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86788cee-7812-46aa-ac71-417b6bcca8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_lasso = {\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "}\n",
    "\n",
    "grid_lasso = GridSearchCV(\n",
    "    lasso_pipe,\n",
    "    param_grid=param_lasso,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_lasso.fit(X_train, y_train)\n",
    "print(\"Best params Lasso (Logit L1):\", grid_lasso.best_params_)\n",
    "print(\"Best CV score (F1)         :\", grid_lasso.best_score_)\n",
    "\n",
    "best_lasso = grid_lasso.best_estimator_\n",
    "eval_model(\"Lasso Logistic (L1)\", best_lasso, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff2aa0-d5de-4064-9215-d468b4e4cc36",
   "metadata": {},
   "source": [
    "Le modèle Lasso logistique reprend la même idée que la régression logistique mais ajoute une pénalité L1 sur les coefficients, ce qui force certains d’entre eux à devenir exactement nuls.\n",
    "Concrètement, il sert à faire de la sélection automatique de variables en conservant surtout les signaux les plus pertinents dans les features, et le GridSearch fait varier le paramètre de régularisation pour afficher les performances (Accuracy, F1, ROC‑AUC) du meilleur compromis entre complexité du modèle et qualité de prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c4aab-a06f-400d-822f-b9d7fe10a182",
   "metadata": {},
   "source": [
    "#### 5. Bridge / ElasticNet (L1 + L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e990074-6cde-48a6-bd08-d5b0f118b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_enet = {\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "    \"clf__l1_ratio\": [0.1, 0.5, 0.9]  # 0.1 ~ plus L2, 0.9 ~ plus L1\n",
    "}\n",
    "\n",
    "grid_enet = GridSearchCV(\n",
    "    enet_pipe,\n",
    "    param_grid=param_enet,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_enet.fit(X_train, y_train)\n",
    "print(\"Best params ElasticNet:\", grid_enet.best_params_)\n",
    "print(\"Best CV score (F1)    :\", grid_enet.best_score_)\n",
    "\n",
    "best_enet = grid_enet.best_estimator_\n",
    "eval_model(\"ElasticNet Logistic\", best_enet, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3fdba-bff0-4a9d-8f06-3abec84ca12f",
   "metadata": {},
   "source": [
    "Le modèle ElasticNet (Bridge dans contexte) combine une pénalité L1 et une pénalité L2 afin de bénéficier à la fois de la sélection de variables (comme Lasso) et de la stabilisation des coefficients (comme Ridge).\n",
    "Il sert à contrôler plus finement la régularisation via le paramètre l1_ratio, et le GridSearch teste plusieurs niveaux de régularisation pour afficher le jeu de paramètres qui maximise le F1‑score tout en gardant un modèle robuste dans un environnement financier potentiellement bruité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0349ea-6f3d-4f49-b9f3-a672fa20f2da",
   "metadata": {},
   "source": [
    "#### 6. Bayesian Ridge (version classification simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc556d6a-84ab-4cfe-8808-15c33c4e6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class BayesianRidgeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = BayesianRidge(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # y en 0/1\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # approx: output = mean prediction, variance ignorée\n",
    "        y_hat = self.model.predict(X)\n",
    "        # clip pour rester dans [0,1]\n",
    "        y_hat = np.clip(y_hat, 0, 1)\n",
    "        return np.vstack([1 - y_hat, y_hat]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "bayes_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", BayesianRidgeClassifier())\n",
    "])\n",
    "\n",
    "param_bayes = {\n",
    "    # paramètres éventuels de BayesianRidge si tu veux tuner\n",
    "    \"clf__model__alpha_1\": [1e-6, 1e-4],\n",
    "    \"clf__model__alpha_2\": [1e-6, 1e-4],\n",
    "    \"clf__model__lambda_1\": [1e-6, 1e-4],\n",
    "    \"clf__model__lambda_2\": [1e-6, 1e-4],\n",
    "}\n",
    "\n",
    "grid_bayes = GridSearchCV(\n",
    "    bayes_pipe,\n",
    "    param_grid=param_bayes,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_bayes.fit(X_train, y_train)\n",
    "print(\"Best params BayesianRidge:\", grid_bayes.best_params_)\n",
    "print(\"Best CV score (F1)      :\", grid_bayes.best_score_)\n",
    "\n",
    "best_bayes = grid_bayes.best_estimator_\n",
    "eval_model(\"Bayesian Ridge (Classifier wrapper)\", best_bayes, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c82b6-a05d-4c1a-a289-60380ac4bb6b",
   "metadata": {},
   "source": [
    "Le modèle bayésien utilisé repose sur une régression de type Bayesian Ridge adaptée en classifieur pour une cible binaire, ce qui signifie qu’au lieu de produire un seul vecteur de coefficients, il modélise une distribution a priori et a posteriori sur ces coefficients.\n",
    "Il sert surtout à quantifier l’incertitude sur les paramètres du modèle et donc implicitement sur les prédictions, et le GridSearch fait varier les hyperparamètres de la prior (par exemple alpha_1, lambda_1) tout en affichant les performances de classification finales (Accuracy, F1, ROC‑AUC) après conversion en probabilités."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6db8c3-15f7-44f8-930f-0780355d45a5",
   "metadata": {},
   "source": [
    "#### 7. XGBoost (sans ensembles scikit‑learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4987e4-bb82-46a5-9b90-adfda3e38ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=300,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_xgb = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"gamma\": [0, 1]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb_clf,\n",
    "    param_grid=param_xgb,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "print(\"Best params XGBoost:\", grid_xgb.best_params_)\n",
    "print(\"Best CV score (F1):\", grid_xgb.best_score_)\n",
    "\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "eval_model(\"XGBoost\", best_xgb, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e64899-568a-4cce-bd1b-6eb988aeaae2",
   "metadata": {},
   "source": [
    "XGBoost est un modèle non linéaire basé sur un ensemble séquentiel d’arbres de décision où chaque nouvel arbre corrige les erreurs du précédent, ce qui le rend très puissant pour capturer des relations complexes et des interactions entre les variables.​\n",
    "Dans ton projet, il sert à explorer des structures de dépendance plus riches entre les retours/volumes historiques et le signe du retour futur, et le GridSearch fait varier des hyperparamètres clés (profondeur des arbres, learning rate, subsample, colsample, gamma) pour afficher les meilleurs scores de classification sur les données de validation et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4e5bf-219e-41d4-9a16-646dc56d8853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
